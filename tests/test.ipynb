{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_4bit_type(typename, device=None, blocksize=64):\n",
    "    if device is None: device = 'cuda'\n",
    "    data = None\n",
    "    if typename == 'nf4':\n",
    "        ''' Implements the NF4 data type.\n",
    "\n",
    "            Constructs a quantization data type where each bin has equal area under a standard normal distribution N(0, 1) that\n",
    "            is normalized into the range [-1, 1].\n",
    "\n",
    "            For more information read the paper: QLoRA: Efficient Finetuning of Quantized LLMs (https://arxiv.org/abs/2305.14314)\n",
    "\n",
    "            Implementation of the NF4 data type in bitsandbytes can be found in the `create_normal_map` function in\n",
    "            the `functional.py` file: https://github.com/TimDettmers/bitsandbytes/blob/main/bitsandbytes/functional.py#L236.\n",
    "        '''\n",
    "        data = [-1.0, -0.6961928009986877, -0.5250730514526367, -0.39491748809814453, -0.28444138169288635,\n",
    "                -0.18477343022823334, -0.09105003625154495, 0.0, 0.07958029955625534, 0.16093020141124725,\n",
    "                0.24611230194568634, 0.33791524171829224, 0.44070982933044434, 0.5626170039176941,\n",
    "                0.7229568362236023, 1.0]\n",
    "    elif typename == 'fp4':\n",
    "        # 0b000 = 0\n",
    "        # 0b001 = 0.0625\n",
    "        # 0b010 = 8\n",
    "        # 0b011 = 12\n",
    "        # 0b100 = 4\n",
    "        # 0b101 = 6\n",
    "        # 0b110 = 2\n",
    "        # 0b111 = 3\n",
    "        # can also be created with bnb.functional.create_fp8_map(signed=True, exponent_bits=2, precision_bits=1, total_bits=4)\n",
    "        data = [0, 0.0625, 8.0, 12.0, 4.0, 6.0, 2.0, 3.0, -0, -0.0625, -8.0, -12.0, -4.0, -6.0, -2.0, -3.0]\n",
    "    elif typename == 'int4':\n",
    "        data = [7, 6, 5, 4, 3, 2, 1, 0, -0, -1, -2, -3, -4, -5, -6, -7]\n",
    "    elif typename == 'af4':\n",
    "        # Taken from: NF4 Isn't Information Theoretically Optimal (and that's Good)\n",
    "        # https://arxiv.org/abs/2306.06965\n",
    "        if blocksize == 64:\n",
    "            data = [-1., -0.69441008, -0.51243739, -0.3736951, -0.25607552, -0.14982478,\n",
    "                    -0.04934812,  0., 0.04273164, 0.12934483, 0.21961274, 0.31675666,\n",
    "                    0.42563882,  0.55496234,  0.72424863,  1.][::-1]\n",
    "        else:\n",
    "            raise NotImplementedError('4-bit AbnormalFloats currently only support blocksize 64.')\n",
    "\n",
    "    if data is None:\n",
    "        raise NotImplementedError(f'Typename {typename} not supported')\n",
    "\n",
    "    data = Tensor(data)\n",
    "    data /= data.abs().max()\n",
    "    assert data.numel() == 16\n",
    "\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
       "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_4bit_type('nf4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
